== scaling
- The search queue grows too large, so maybe the threads should stop populating the queue after for example height=3 and keep working on an internal queue that doesn't grow much.
- A theory is maybe complete search tasks do not scale but positive ones do? NO, they also stop scaling after 2nd thread.

== bugs
- proj produces universal predicates by leaving unbound vars out. Unbound vars shouldn't be able to be left out. For example, proj(and(cons, proj(cons, {n0->n1, n4->n5})), {n2->n6, n3->n7, n1->n8})
- reverse/2 isn't found. it founds a foldr variant if example is {b0:[], as:[1,2,3], b:[3,2,1]} and Stack is used instead of a Queue. 
- static counters like argNameCounter on NameVar are not thread-safe.
FIXED - 1 thread fails more tests than more threads
- AND operator should insert constaints for NameVars
- AND operator should(?) allow operands with no common columns?
- 4 threads sometimes does not terminate.
FIXED - assertSingleResult calls FindAllPrograms which takes a long time and often doesn't terminate. Switch to assertFirstResult for most tests.

== To be discussed
- see 'exference' for their approach to universal predicates: https://github.com/lspitzner/exference/
- both fold's own, recursive and base parameters' names are ground and arbitrary. maybe it's possible to derive these names or make them dynamic by inferring which argument is bound to what using their types? This may apply to CNP language in general by removing argument name mechanics if types are specific enough. 
- Hamfelt Nilsson folds are unfamiliar to functional programmers, maybe add some akin to those in FP?
- The search always works with the first hole, searching top-down. If the first hole can't be filled, it disposes of the search node. How does this affect the search exhaustiveness? Maybe it should try each hole in a separate search path.
- The search always works with the first hole (top-down, left-right). Is avoiding other orders eliminating some results?
- For an observation such as {{X:1, Y:F1, Z:[1,2,3]}, {X:2, Y:F2, Z:F1}, {X:3, Y:F3, Z:F3}} with a valence {X:in, Y:out, Z:in}, cons might bind X=a,Z=ab

== Improvements
- Synthesizing Reverse2FoldR requires proj to be able to leave constant [] unbound to the head. Need some way to let proj leave them unbound while flagging the arg in subsequent search to be ground. 
- synthesizing member is difficult because it requires lifting the element variable which results in a pretty ugly and unreadable code, including changing the operator semantics. One solution is to have a closure environment containing only the 'in' arguments of the root expression, practically having an expression-scope constant lookup table. These are hardly variables since restricting to 'in' arguments assures that they are ground to begin with, therefore constants, and they can only appear in the leafs of the predicate expression as in lifted(element). for example, member would become
          member : {a, as} = any(lifted(a)) : {a, as}
          any(P)([A|L]) :- P(A).
          any(P)([_|L]) :- any(P)(L).
- ObservedProgram, ValenceVar, AlphaRelation should be folded into one class. Redundancy of names in ValenceVar and AlphaRelation is algorithmically problemmatic.
- If fold names can be same as FOLD's P, standard fold-2 and fold-3 programs would be faster since they wouldn't fall into the back of a proj line.
- AND's left and right valences might be grouped under the distinct lh valence. because if that lh valence won't bind, all the rh ones are dead search paths.


- When workers produce work items, they may keep some to themselves in order to avoid putting them back onto the queue only to get them back again. This hurts the search order, but might improve synchronization time. Or, they may not keep to themselves but insert to queue in groups of N, and dequeue in groups of N.
- The limiting factor to scaling may not be synchronization but memory. Memory grows quite fast to 10GB.
FXD - Interlocked.Increment probably drops CPU core cashes so indirectly causes synchronization time. RESULT: No improvement in scaling, only minor general improvement.
- candidates can be put on a separate collection according to their depth, so shallower candidates get processed first.
- double projections such as (proj(proj(cons, ...), ...) should be avoided.
- identity projections such as (proj(cons, a->a, b->b, ab->ab)) should be avoided.
- Importing from job files like the CNP that shipped with RICE. Maybe use JSON files instead.
- Extensibility. Besides the CNP.CoreLanguage package (id, cons, const, and, or, foldr, foldl, map) there may be a CNP.Numerics or CNP.Math as well as CNP.Geometry (for avionics stuff). When running a job you can run the full suite by default, or choose only some for efficiency (Job.SetSearchOptions(CNP.CoreLanguage | CNP.Math) or something like that). This might require some packaging system decisions. Moreover we can allow the client to add their operators by giving a type that inherits from CNP.Program and a type for the reverse semantics (or a static method like we currently have).
- AND operator: Constraint on ObservableProgram to be different than another program. Useful for and(P, Q). P dif Q is a requirement for efficiency. Once P is bound to, p, Q will be changed to be different to p. When grounding Q, if the new operator is different than p, the constraint is abandoned. if it's same as p, constrained is inherited to Q's arguments as P.i diff to Q.i for all arguments i. Custom constraints like this should possible to add easily without breaking core synthesis. Maybe a Program node has a Constraints list that is manipulated during search and ultimately required to be empty for program to be accepted.
- Constraint for the proj operator to be not repeated.
- Instead of Dictionary<int, IEnumerable<ProgramType>>, Lookup can be used. It's immutable but it might suffice.
- AndOrValence generation can be more efficient by using bit operations and truth tables for finding combinations
- The CreateAtFirstHole parameter type can be changed from Program to IProgramSource that allows an immutable interface and cloning. Useful for extensibility.
- Having a ObservedProgram.ReplaceInPlace(newProgram) would be helpful to eliminate one cloning in the implementations of CreateAtFirstHole
- A benchmarking system that can run the tests wrt thread count and report the plots as latex code.
- Pretty-printing system to replace the use of ToString
- AndOrValence and FoldValence inheriting from Valence and still having it as a sort-of component is not healthy abstraction.

== TidyUp
- foldr and foldl Qs names, and id's names should be {a, a'}, or {e1, e2}, not {a, b} because their types are the same.
- Would a context reduce variable mistreatment bugs in the future? An AlphaTuple would have a FreeContext, 
  and if other AlphaTuples want to share its context they'd need to be created through its method
  .InSameContext((name, value),...,(mame,value)) : AlphaTuple.
  The same context would apply to program components, so that you can't mix and match program components with different contexts. For example it'd be a problem with CloneAndReplace if the new component is from a different context.
- Collate observables in test results into a single row in table.
- Test results measurements last one should be in main directory. Also in a historical Json data file.

== Optimization
- Typestore can use multiple indexes at the cost of memory and initialization time. It already indexes by the number of ins and outs, but it could also index by each combination of ground argument names.
- Search is branching for each valence, for example P{a:in, b:out} and P{a:in, b:in}. Sub-searches for each of these may have overlap.
- As search decomposes, it populates redundant holes for the same space, like {a:in, b:out} and {a:in, b:in} separately (for example, for the base case of folds). This gives two separate search paths, but if for example 'id' is not working for this hole, it won't work for either of them. This inefficiency could be improved by grouping multiple valences into one search path. One potential pitfall is to combine valences of component expressions ina functional valence, as they're only searched for that particular valence in the given context. But, for example, for decomposing proj, valences might be able to grouped.
- Fold synthesis doesn't need to unfold the examples for each valence.